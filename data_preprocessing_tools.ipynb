{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vjhawar12/IrisClassifier/blob/main/data_preprocessing_tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37puETfgRzzg"
      },
      "source": [
        "# Data Preprocessing Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoRP98MpR-qj"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "veKnkJQG2no1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RopL7tUZSQkT"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(\"Data.csv\")\n",
        "\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values\n",
        "\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWbf6ZqL2_R1",
        "outputId": "61aac857-cae9-42cc-ce81-884619b2514b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uO-I_caQ3M3C",
        "outputId": "dbce1dac-25d2-4741-faf8-c3d65c0b1756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfKXNxlSabC"
      },
      "source": [
        "## Taking care of missing data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# creating imputer object to replace missing values (np.nan)\n",
        "# with the mean of all the values (strategy=\"mean\")\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
        "\n",
        "# fitting the data across all rows in the 1-2 columns (3 is an upper bound)\n",
        "imputer.fit(X[:, 1:3])\n",
        "\n",
        "# replacing the X object with the fitted data\n",
        "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
        "\n",
        "print(X)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxTtb3OH3RMJ",
        "outputId": "28c6abe9-2bfb-4090-e6d5-4bc98ec5a20b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CriG6VzVSjcK"
      },
      "source": [
        "## Encoding categorical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhSpdQWeSsFh"
      },
      "source": [
        "### Encoding the Independent Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to encode the strings we need to import these two classes to perform OneHotEncoding (converting to binary vector) on the first column\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# name='encoder' (type of transformation) transformers=OneHotEncoder (what we're using for transformation), columns=[0] (just transform the 0th column)\n",
        "# remainder=passthrough means do not affect the other columns that are not 0\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
        "\n",
        "# We're trying to transform X (which is all the rows and columns 0-2)\n",
        "X = np.array(ct.fit_transform(X)) # this results the encoded table\n",
        "\n",
        "print(X)"
      ],
      "metadata": {
        "id": "s9xrplh1RnNf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "851406ce-36f8-447f-dbdf-e7e41382bba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eru4SrBQpxwK",
        "outputId": "95245ab9-b01b-4f16-8c67-4c0670e11d8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXh8oVSITIc6"
      },
      "source": [
        "### Encoding the Dependent Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "print(y)"
      ],
      "metadata": {
        "id": "84TFuUzqyA13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc42321c-07ea-4530-f99d-2cb57bffa720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb_vcgm3qZKW"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step the data will be split into seperate sets. In the training set, you train the model based on existing observations. In the test set you evaluate the performance of your model based on new observations. Feature scaling needs to be done after splitting the dataset so that you can make sure the test set (which is to remain unaffected) is not impacted by the feature scaling.\n",
        "\n",
        "In this step, we are basically splitting the data into 4 sets--not 2. We will get Xtest (the features of the test set), XTrain (the features of the training set), yTrain (the dependent varaible of the training set), and yTest (the features of the test set).\n",
        "\n",
        "The machine learning model that we are going to build will require all of this data."
      ],
      "metadata": {
        "id": "Bc4_AugA6fiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# here the data is being split into the 4 sets. Test_size means 80% training, 20% test.\n",
        "# Random_state=1 means the data will be split in the same manner each time\n",
        "XTrain, XTest, yTrain, yTest = train_test_split(X, y, test_size=0.2, random_state=1)"
      ],
      "metadata": {
        "id": "YhUJjL9p9KxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(XTrain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYBlRK7o-D_A",
        "outputId": "c44ed0c3-628a-4338-a008-aba97c4eec9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(XTest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsXyEoaL-FH-",
        "outputId": "6f1e2a57-811f-4148-dd6e-2f92a548b495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(yTrain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHsccD_9-GYl",
        "outputId": "a2c571d6-afff-43ab-fc4a-9bbf673684c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(yTest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20hECP6w-I5p",
        "outputId": "92552bd4-8dea-410d-f0f5-7c058bf32b4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpGqbS4TqkIR"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling allows us to put all of our features on the same scale. This is so that some of our data doesn't get ignored by the machine learning models.\n",
        "\n",
        "Feature scaling isn't always neccessary.\n",
        "\n",
        "There are two methods for feature scaling: Standarization and normalization. In standarization the features range between -3 and 3. In normalization, the features range between 0 and 1.\n",
        "\n",
        "Use normalization when there is a fairly normal distribution in most of your features. Standarization works well all of the time. When in doubt, go for standarization.\n",
        "\n",
        "Standarization is calculated like this: (x - mean(x)) / standardDeviation(x)\n",
        "\n",
        "We will have to limit the mean value to just the training set since the test set is supposed to be unaltered."
      ],
      "metadata": {
        "id": "DYXv2P13COLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScalar # this library is for standarization\n",
        "\n",
        "sc = StandardScalar()\n"
      ],
      "metadata": {
        "id": "SHJcVbmiQF6P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}